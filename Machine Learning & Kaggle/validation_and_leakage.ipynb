{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and Data Leakage Lab\n",
    "\n",
    "One of the most common mistakes in machine learning is improper validation. This can lead to models that seem great during development but fail in the real world.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Why proper validation matters\n",
    "- How to use cross-validation\n",
    "- What data leakage is and how to avoid it\n",
    "- Common mistakes that cause leakage\n",
    "\n",
    "**Time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.datasets import fetch_california_housing, load_breast_cancer\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Why Validation Matters\n",
    "\n",
    "The goal of ML is to build models that work on **new, unseen data**. If we only evaluate on training data, we can't tell if our model is actually learning or just memorizing.\n",
    "\n",
    "### Overfitting Demo\n",
    "\n",
    "Let's see what happens when we evaluate on training data vs test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Train a very deep decision tree (prone to overfitting)\n",
    "overfit_model = DecisionTreeRegressor(max_depth=None, random_state=42)  # No depth limit!\n",
    "overfit_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on training data\n",
    "train_score = overfit_model.score(X_train, y_train)\n",
    "print(f\"Training R²: {train_score:.4f}\")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_score = overfit_model.score(X_test, y_test)\n",
    "print(f\"Test R²:     {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wow!** The model gets nearly perfect score on training data but much worse on test data. This is **overfitting** — the model memorized the training data instead of learning general patterns.\n",
    "\n",
    "If we had only looked at training performance, we'd think we had a great model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with a simpler model\n",
    "simple_model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "simple_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Simple Decision Tree (max_depth=5):\")\n",
    "print(f\"  Training R²: {simple_model.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R²:     {simple_model.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simpler model has lower training score but similar (or better!) test score. It generalizes better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Cross-Validation\n",
    "\n",
    "A single train/test split can be unreliable — results depend on which data ends up in which set. **Cross-validation** solves this by using multiple splits.\n",
    "\n",
    "### K-Fold Cross-Validation\n",
    "\n",
    "The data is split into K parts (\"folds\"). We train K times, each time using a different fold as the test set:\n",
    "\n",
    "```\n",
    "Fold 1: [TEST] [train] [train] [train] [train]\n",
    "Fold 2: [train] [TEST] [train] [train] [train]\n",
    "Fold 3: [train] [train] [TEST] [train] [train]\n",
    "Fold 4: [train] [train] [train] [TEST] [train]\n",
    "Fold 5: [train] [train] [train] [train] [TEST]\n",
    "```\n",
    "\n",
    "This gives us 5 scores instead of 1, and every data point gets to be in the test set exactly once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple cross-validation with sklearn\n",
    "model = LinearRegression()\n",
    "\n",
    "# 5-fold cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "\n",
    "print(\"5-Fold Cross-Validation Results:\")\n",
    "print(f\"  Scores: {scores}\")\n",
    "print(f\"  Mean:   {scores.mean():.4f}\")\n",
    "print(f\"  Std:    {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a more reliable estimate of model performance:\n",
    "- **Mean** tells us the expected performance\n",
    "- **Std** tells us how much it varies (lower is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models using cross-validation\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree (depth=5)': DecisionTreeRegressor(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "print(\"Model Comparison (5-fold CV):\")\n",
    "print(f\"{'Model':<25} {'Mean R²':<10} {'Std':<10}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "    print(f\"{name:<25} {scores.mean():<10.4f} {scores.std():<10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Data Leakage\n",
    "\n",
    "**Data leakage** occurs when information from outside the training data is used to create the model. This leads to overly optimistic performance estimates that don't hold up in reality.\n",
    "\n",
    "### Common Causes of Leakage\n",
    "\n",
    "1. **Preprocessing before splitting** — Fitting scalers/encoders on all data\n",
    "2. **Target leakage** — Features that contain information about the target that wouldn't be available at prediction time\n",
    "3. **Train-test contamination** — Test data somehow influencing training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Preprocessing Leakage\n",
    "\n",
    "Let's see what happens when we scale data incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG WAY: Scale before splitting\n",
    "print(\"=== WRONG: Scale before split ===\")\n",
    "\n",
    "scaler_wrong = StandardScaler()\n",
    "X_scaled_wrong = scaler_wrong.fit_transform(X)  # Fit on ALL data - LEAKAGE!\n",
    "\n",
    "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(\n",
    "    X_scaled_wrong, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_wrong = LinearRegression()\n",
    "model_wrong.fit(X_train_w, y_train_w)\n",
    "score_wrong = model_wrong.score(X_test_w, y_test_w)\n",
    "print(f\"Test R²: {score_wrong:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIGHT WAY: Split first, then scale\n",
    "print(\"\\n=== RIGHT: Split first, then scale ===\")\n",
    "\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_right = StandardScaler()\n",
    "X_train_scaled = scaler_right.fit_transform(X_train_r)  # Fit only on training data\n",
    "X_test_scaled = scaler_right.transform(X_test_r)         # Transform test data\n",
    "\n",
    "model_right = LinearRegression()\n",
    "model_right.fit(X_train_scaled, y_train_r)\n",
    "score_right = model_right.score(X_test_scaled, y_test_r)\n",
    "print(f\"Test R²: {score_right:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the difference is small because Linear Regression isn't very sensitive to scaling. But for other algorithms (like neural networks or SVMs), improper scaling can cause significant leakage.\n",
    "\n",
    "**The principle is important:** Always split first, then preprocess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Target Leakage (The Dangerous Kind)\n",
    "\n",
    "Target leakage is when a feature directly or indirectly contains information about the target. Let's create a dramatic example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset (classification)\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "y_cancer = cancer.target  # 0 = malignant, 1 = benign\n",
    "\n",
    "print(f\"Original features: {X_cancer.shape[1]}\")\n",
    "print(f\"Target distribution: {np.bincount(y_cancer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a \"leaky\" feature: treatment_started\n",
    "# In reality, this would only be known AFTER diagnosis (our target)\n",
    "# So it shouldn't be used as a feature for prediction\n",
    "\n",
    "# Create a leaky feature: \"treatment started\" is highly correlated with being diagnosed\n",
    "np.random.seed(42)\n",
    "X_leaky = X_cancer.copy()\n",
    "# Malignant cases (y=0) mostly have treatment started\n",
    "# Benign cases (y=1) mostly don't have treatment\n",
    "X_leaky['treatment_started'] = np.where(\n",
    "    y_cancer == 0,\n",
    "    np.random.choice([0, 1], size=len(y_cancer), p=[0.1, 0.9]),  # 90% of malignant have treatment\n",
    "    np.random.choice([0, 1], size=len(y_cancer), p=[0.9, 0.1])   # 10% of benign have treatment\n",
    ")\n",
    "\n",
    "print(\"Created 'treatment_started' feature (THIS IS LEAKY!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train_l, X_test_l, y_train_l, y_test_l = train_test_split(\n",
    "    X_leaky, y_cancer, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train with the leaky feature\n",
    "model_leaky = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "model_leaky.fit(X_train_l, y_train_l)\n",
    "\n",
    "print(\"=== Model WITH Leaky Feature ===\")\n",
    "print(f\"Training Accuracy: {model_leaky.score(X_train_l, y_train_l):.4f}\")\n",
    "print(f\"Test Accuracy:     {model_leaky.score(X_test_l, y_test_l):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at feature importances\n",
    "importance_leaky = pd.DataFrame({\n",
    "    'Feature': X_leaky.columns,\n",
    "    'Importance': model_leaky.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(10)\n",
    "\n",
    "print(\"\\nTop 10 Feature Importances:\")\n",
    "print(importance_leaky)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leaky feature (`treatment_started`) is the most important! The model is essentially \"cheating\" by using information that wouldn't be available in a real prediction scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train without the leaky feature\n",
    "X_clean = X_cancer.copy()  # No leaky feature\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_clean, y_cancer, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_clean = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "model_clean.fit(X_train_c, y_train_c)\n",
    "\n",
    "print(\"=== Model WITHOUT Leaky Feature ===\")\n",
    "print(f\"Training Accuracy: {model_clean.score(X_train_c, y_train_c):.4f}\")\n",
    "print(f\"Test Accuracy:     {model_clean.score(X_test_c, y_test_c):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clean model has a lower score, but it's an **honest** score. This is what you'd actually get in production.\n",
    "\n",
    "### How to Detect Target Leakage\n",
    "\n",
    "Warning signs:\n",
    "- **Unrealistically high performance** (especially if you're new to the problem)\n",
    "- **A single feature dominates** importance\n",
    "- **Feature that seems too good** — ask \"would I have this at prediction time?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Proper Validation Pipeline\n",
    "\n",
    "Here's a checklist for proper validation:\n",
    "\n",
    "### ✅ Validation Checklist\n",
    "\n",
    "1. **Split data first** — Before any preprocessing\n",
    "2. **Fit preprocessors on training data only** — Then transform both train and test\n",
    "3. **Check for leaky features** — Would this info be available at prediction time?\n",
    "4. **Use cross-validation** — Don't rely on a single split\n",
    "5. **Keep a holdout test set** — Final evaluation on data never used during development\n",
    "6. **Be skeptical of great results** — If it seems too good, it probably is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a proper pipeline using sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a pipeline that handles preprocessing correctly\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Cross-validation automatically handles the train/test split correctly for each fold\n",
    "scores = cross_val_score(pipeline, X_cancer, y_cancer, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"Proper Pipeline with Cross-Validation:\")\n",
    "print(f\"  Scores: {scores}\")\n",
    "print(f\"  Mean:   {scores.mean():.4f}\")\n",
    "print(f\"  Std:    {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Pipeline ensures that preprocessing is done correctly within each cross-validation fold — the scaler is fit only on the training portion of each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Concept | What it means |\n",
    "|---------|---------------|\n",
    "| **Overfitting** | Model works on training data but fails on new data |\n",
    "| **Cross-validation** | Multiple train/test splits for reliable evaluation |\n",
    "| **Data leakage** | Using information that wouldn't be available in production |\n",
    "| **Pipeline** | Ensures preprocessing is done correctly |\n",
    "\n",
    "### Rules to Remember\n",
    "\n",
    "1. **Never evaluate only on training data**\n",
    "2. **Split first, preprocess second**\n",
    "3. **Fit on train, transform on both**\n",
    "4. **If results seem too good, investigate**\n",
    "5. **Use pipelines to avoid mistakes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Cross-validation experiment:** Try different values of `cv` (3, 5, 10). How does the mean and std change?\n",
    "\n",
    "2. **Create your own leaky feature:** Add a feature that's derived from the target (e.g., `y + small_noise`). See how it affects the model.\n",
    "\n",
    "3. **Pipeline practice:** Create a pipeline with a scaler and Random Forest. Use cross-validation to evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Ready to submit to Kaggle? Continue to the **[Kaggle Submission Lab](./kaggle_submission.ipynb)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
